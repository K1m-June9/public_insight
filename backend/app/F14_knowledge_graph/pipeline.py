import asyncio
import logging
import os
import fitz
import re
from elasticsearch import Elasticsearch
from typing import Dict, List, Any, Tuple

# --- ë°ì´í„° ìœµí•©ì„ ìœ„í•œ ì‘ì—… ---
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from kiwipiepy import Kiwi

# --- SQLAlchemy ë¹„ë™ê¸° ì„¤ì • ---
from sqlalchemy import select
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# --- í”„ë¡œì íŠ¸ ëª¨ë¸ ì„í¬íŠ¸ ---
# (settingsì™€ ëª¨ë¸ ê²½ë¡œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
from app.F5_core.config import settings
from app.F7_models.users import User
from app.F7_models.organizations import Organization
from app.F7_models.categories import Category
from app.F7_models.feeds import Feed
from app.F7_models.bookmarks import Bookmark
from app.F7_models.ratings import Rating

# --- ë¡œê¹… ì„¤ì • ---
logger = logging.getLogger(__name__)

# --- Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
# ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ, ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©í•¨.
kiwi = Kiwi()

# --- ë°ì´í„° êµ¬ì¡° ì •ì˜ (Type Hinting) ---
MysqlData = Dict[str, List[Dict[str, Any]]]
PdfTextData = Dict[int, str]
SearchLogData = List[Tuple[str, str]]
TransformedData = Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] # (nodes, relationships)

# --- í˜•íƒœì†Œ ë¶„ì„ê¸°(ì¼ë‹¨ í•œêµ­ì–´ ì „ìš©ì´ë¼ëŠ”ë° ê¸°íƒ€ ì„¤ì • ë“±ì€ ì•ˆí•œ ìƒíƒœ) ---
def kiwi_tokenizer(text: str) -> List[str]:
    """
    (Tokenizer) Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    - TfidfVectorizerì˜ tokenizerë¡œ ì‚¬ìš©ë  ê²ƒì„.
    """
    # 1. kiwi.tokenize()ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰
    tokens = kiwi.tokenize(text)
    # 2. í’ˆì‚¬ê°€ 'ì¼ë°˜ ëª…ì‚¬(NNG)' ë˜ëŠ” 'ê³ ìœ  ëª…ì‚¬(NNP)'ì¸ í† í°ë§Œ ì¶”ì¶œí•¨.
    # 3. ì¶”ê°€ì ìœ¼ë¡œ, í•œ ê¸€ìì§œë¦¬ ëª…ì‚¬ëŠ” ì˜ë¯¸ ì—†ëŠ” ê²½ìš°ê°€ ë§ì•„ ì œì™¸í•¨ (ì˜ˆ: 'ê²ƒ', 'ìˆ˜', 'ë“±').
    return [
        token.form for token in tokens 
        if token.tag in {'NNG', 'NNP'} and len(token.form) > 1
    ]


# ---------- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ í•­ìƒ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì°¾ë„ë¡ ì´ˆê¸°ì— ì„¤ì • ----------
# 1. í˜„ì¬ ì´ íŒŒì¼(pipeline.py)ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì°¾ìŒ
#    ex: /home/pumpkinbee/public_insight/app/F14_knowledge_graph/pipeline.py
current_file_path = os.path.abspath(__file__)

# 2. F14_knowledge_graph í´ë”ì˜ ê²½ë¡œë¥¼ ì°¾ìŒ (í•œ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight/app/F14_knowledge_graph
f14_dir = os.path.dirname(current_file_path)

# 3. app í´ë”ì˜ ê²½ë¡œë¥¼ ì°¾ìŒ (ë‘ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight/app
app_dir = os.path.dirname(f14_dir)

# 4. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ ì°¾ìŒ (ì„¸ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight
project_root_dir = os.path.dirname(app_dir)

# 5. ë£¨íŠ¸ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ static í´ë”ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒì„±í•¨
#    ì´ë ‡ê²Œ í•˜ë©´ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  í•­ìƒ ë™ì¼í•œ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ë¦¬í‚´
PDF_BASE_PATH = os.path.join(project_root_dir, "backend", "static", "feeds_pdf")

# --- MySQL ì—°ê²° ì„¤ì • (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©) ---
DATABASE_URL = (
    f"mysql+aiomysql://{settings.DB_USER}:{settings.DB_PASSWORD}@"
    f"{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"
)
engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

# =================================== EXTRACT ===================================
async def phase_extract(db: AsyncSession) -> Tuple[MysqlData, PdfTextData, SearchLogData]:
    """
    ETL íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„: Extract
    - ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤(MySQL, PDF, Elasticsearch)ì—ì„œ ì›ë³¸ ë°ì´í„° ì¶”ì¶œí•¨.
    - ë°ì´í„° ê°€ê³µ ì—†ì´, ìˆëŠ” ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì— ì§‘ì¤‘í•¨.
    """
    logger.info("--- Phase 1: Extract ì‹œì‘ ---")

    # 1.1: MySQLì—ì„œ ë°ì´í„° ì¶”ì¶œ
    mysql_data = await _extract_from_mysql(db)
    feed_count = len(mysql_data.get('feeds', []))
    logger.info(f"MySQLì—ì„œ {feed_count}ê°œì˜ í”¼ë“œ í¬í•¨, ì „ì²´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ.")

    # 1.2: PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # ğŸ”§ ìˆ˜ì •: content_typeì´ 'PDF'ì¸ í”¼ë“œë§Œ í•„í„°ë§í•˜ì—¬ ì „ë‹¬í•¨
    feeds_with_pdf = [
        f for f in mysql_data.get('feeds', []) 
        if f.get('content_type') == 'pdf' and f.get('pdf_file_path')
    ]
    pdf_texts = _extract_text_from_pdfs(feeds_with_pdf)
    logger.info(f"{len(pdf_texts)}ê°œì˜ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
    
    # 1.3: Elasticsearchì—ì„œ ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ
    search_logs = _extract_search_logs_from_es()
    logger.info(f"Elasticsearchì—ì„œ {len(search_logs)}ê°œì˜ ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ ì™„ë£Œ.")

    logger.info("--- Phase 1: Extract ì¢…ë£Œ ---")
    
    return mysql_data, pdf_texts, search_logs


async def _extract_from_mysql(db: AsyncSession) -> MysqlData:
    """
    (Helper) SQLAlchemyë¥¼ ì‚¬ìš©í•˜ì—¬ MySQLì˜ 6ê°œ í•µì‹¬ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´.
    - Feed í…Œì´ë¸”ì—ì„œ content_typeê³¼ original_textë¥¼ ì¶”ê°€ë¡œ ì¡°íšŒí•¨.
    """
    user_res = await db.execute(select(User.id, User.user_id, User.nickname))
    org_res = await db.execute(select(Organization.id, Organization.name))
    cat_res = await db.execute(select(Category.id, Category.name, Category.organization_id))
    
    # ğŸ”§ ìˆ˜ì •: Feed í…Œì´ë¸” ì¡°íšŒ ì‹œ content_typeê³¼ original_text ì»¬ëŸ¼ ì¶”ê°€
    feed_res = await db.execute(select(
        Feed.id, Feed.title, Feed.summary, 
        Feed.content_type, Feed.pdf_file_path, Feed.original_text,
        Feed.organization_id, Feed.category_id, Feed.published_date
    ))
    
    bookmark_res = await db.execute(select(Bookmark.user_id, Bookmark.feed_id))
    rating_res = await db.execute(select(Rating.user_id, Rating.feed_id, Rating.score))

    data = {
        "users": [dict(r) for r in user_res.mappings().all()],
        "organizations": [dict(r) for r in org_res.mappings().all()],
        "categories": [dict(r) for r in cat_res.mappings().all()],
        "feeds": [dict(r) for r in feed_res.mappings().all()],
        "bookmarks": [dict(r) for r in bookmark_res.mappings().all()],
        "ratings": [dict(r) for r in rating_res.mappings().all()],
    }
    return data


def _extract_text_from_pdfs(feeds: List[Dict[str, Any]]) -> PdfTextData:
    """
    (Helper) PyMuPDF (fitz)ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ëª©ë¡ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•¨.
    - ê° í”¼ë“œì˜ 'pdf_file_path'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ êµ¬ì„±í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì½ìŒ.
    - íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ, í•´ë‹¹ í”¼ë“œëŠ” ê±´ë„ˆë›°ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹€.
    """
    # ğŸ’¥ ì¤‘ìš”: ì´ ê²½ë¡œëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ í•¨.
    # ë³´í†µ í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì„.
    base_path = PDF_BASE_PATH
    extracted_texts = {}

    for feed in feeds:
        feed_id = feed.get('id')
        relative_path = feed.get('pdf_file_path')

        if not feed_id or not relative_path:
            continue

        # UUID íŒŒì¼ëª…ì— .pdf í™•ì¥ìë¥¼ ì¶”ê°€í•˜ì—¬ ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±
        pdf_path = os.path.join(base_path, f"{relative_path}.pdf")

        try:
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(pdf_path):
                logger.warning(f"PDF íŒŒì¼ ì—†ìŒ (ê±´ë„ˆëœ€): {pdf_path}")
                continue
            
            # PDF íŒŒì¼ì„ ì—´ê³  ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²°í•©í•¨
            doc = fitz.open(pdf_path)
            full_text = ""
            for page in doc:
                full_text += page.get_text()
            
            extracted_texts[feed_id] = full_text
            doc.close()

        except Exception as e:
            # PyMuPDF ì²˜ë¦¬ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•¨
            logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {pdf_path} | ì˜¤ë¥˜: {e}")
            continue
            
    return extracted_texts


def _extract_search_logs_from_es() -> SearchLogData:
    """
    (Helper) Elasticsearchì—ì„œ ìµœê·¼ 1ì£¼ì¼ê°„ì˜ ì‚¬ìš©ì ê²€ìƒ‰ ë¡œê·¸ë¥¼ ì¶”ì¶œí•¨.
    - ë¡œê·¸ì¸/ë¹„ë¡œê·¸ì¸ ì‚¬ìš©ìì˜ ëª¨ë“  ê²€ìƒ‰ ë¡œê·¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•¨.
    """
    logger.info("Elasticsearch ì—°ê²° ë° ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ ì‹œì‘...")
    
    try:
        es_client = Elasticsearch(
            "http://elasticsearch:9200",
            basic_auth=(settings.ELASTIC_USERNAME, settings.ELASTIC_PASSWORD)
        )
        if not es_client.ping():
            raise ConnectionError("Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ.")
    except Exception as e:
        logger.error(f"Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return []

    # --- ğŸ”§ [ìˆ˜ì •] ë” ëª…í™•í•˜ê³  ì•ˆì „í•œ Query DSL ---
    # "ì§€ë‚œ 7ì¼ ë™ì•ˆ, ë°±ì—”ë“œ ì»¨í…Œì´ë„ˆì—ì„œ ë‚¨ê¸´, ì •í™•íˆ ê²€ìƒ‰ API ê²½ë¡œì¸ ë¡œê·¸"
    query = {
        "query": {
            "bool": {
                "must": [
                    # ì¡°ê±´ 1: ì»¨í…Œì´ë„ˆ ì´ë¦„ì´ 'backend'
                    {"match": {"container.name.keyword": "backend"}},
                    # ğŸ”§ ì¡°ê±´ 2: URL ê²½ë¡œê°€ ì •í™•íˆ '/api/v1/search'
                    {"match": {"json.url.path": "/api/v1/search"}}
                ],
                "filter": [
                    # ì¡°ê±´ 3: ì‹œê°„ ë²”ìœ„ëŠ” ìµœê·¼ 7ì¼
                    {"range": {"@timestamp": {"gte": "now-7d/d", "lt": "now/d"}}}
                ]
            }
        },
        "size": 10000,
        # ğŸ”§ í•„ìš”í•œ í•„ë“œì— 'json.url.query'ë„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
        "_source": ["json.user.id", "json.url.query", "message"], 
        "sort": [{"@timestamp": {"order": "desc"}}]
    }

    extracted_logs = []
    try:
        response = es_client.search(index="filebeat*", body=query)
        
        for hit in response['hits']['hits']:
            source = hit.get('_source', {})
            # ğŸ”§ json êµ¬ì¡°ê°€ message í•„ë“œ ì•ˆì— ë¬¸ìì—´ë¡œ ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì´ì¤‘ìœ¼ë¡œ íŒŒì‹±
            json_message = source.get('json')
            if not json_message and 'message' in source:
                try: # message í•„ë“œê°€ json í˜•íƒœì¼ ê²½ìš° íŒŒì‹± ì‹œë„
                    import json
                    json_message = json.loads(source['message'])
                except json.JSONDecodeError:
                    continue # json íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€

            if not json_message:
                continue

            # ì´ì œ json_message ì—ì„œ ì•ˆì „í•˜ê²Œ ë°ì´í„° ì¶”ì¶œ
            user_id = json_message.get('user', {}).get('id', 'anonymous') # IDê°€ ì—†ìœ¼ë©´ 'anonymous'
            query_string = json_message.get('url', {}).get('query')

            if not query_string:
                continue

            match = re.search(r'keyword=([^&]+)', query_string)
            if match:
                keyword = match.group(1)
                keyword = re.sub(r'%[0-9a-fA-F]{2}', lambda m: chr(int(m.group(0)[1:], 16)), keyword)
                
                # user_idê°€ ìˆë“  ì—†ë“ (anonymous) ëª¨ë‘ ê²°ê³¼ì— í¬í•¨ì‹œí‚´
                extracted_logs.append((str(user_id), keyword))

    except Exception as e:
        logger.error(f"Elasticsearch ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return []

    return extracted_logs


# =================================== TRANSFORM ===================================
def phase_transform(
    mysql_data: MysqlData, 
    pdf_texts: PdfTextData, 
    search_logs: SearchLogData
) -> TransformedData:
    """
    ETL íŒŒì´í”„ë¼ì¸ 2ë‹¨ê³„: Transform
    - ì¶”ì¶œëœ ì›ë³¸ ë°ì´í„°ë¥¼ 'ì§€ì‹ ê·¸ë˜í”„'ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ë¡œì§.
    - í…ìŠ¤íŠ¸ í†µí•©, í‚¤ì›Œë“œ ì¶”ì¶œ, ìœ ì‚¬ë„ ê³„ì‚° ë“±ì„ ìˆ˜í–‰í•¨.
    """
    logger.info("--- Phase 2: Transform ì‹œì‘ ---")

    # 1. ì¬ë£Œ ì¤€ë¹„: ëª¨ë“  í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë¥¼ í”¼ë“œë³„ë¡œ í†µí•©
    logger.info("1/4: í”¼ë“œë³„ í…ìŠ¤íŠ¸ í†µí•© ì¤‘...")
    all_feed_texts, feed_map = _unify_text_sources(mysql_data, pdf_texts)

    # 2. ë§›ì˜ í•µì‹¬ ì¶”ì¶œ: í˜•íƒœì†Œ ë¶„ì„ ë° TF-IDF ë²¡í„°í™”
    logger.info("2/4: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë²¡í„°í™” ì§„í–‰ ì¤‘...")
    tfidf_matrix, vectorizer = _vectorize_texts(all_feed_texts.values())

    # 3. ìš”ë¦¬ì˜ ì–´ìš¸ë¦¼ ë¶„ì„: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    logger.info("3/4: í”¼ë“œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    similarity_matrix = cosine_similarity(tfidf_matrix)

    # 4. ìµœì¢… í”Œë ˆì´íŒ…: ë…¸ë“œ ë° ê´€ê³„ ë°ì´í„° êµ¬ì¡°í™”
    logger.info("4/4: ìµœì¢… ë…¸ë“œ ë° ê´€ê³„ ë°ì´í„° êµ¬ì¡°í™” ì¤‘...")
    nodes_to_create, relationships_to_create = _structure_graph_data(
        mysql_data, search_logs, feed_map, vectorizer, tfidf_matrix, similarity_matrix
    )
    
    logger.info(f"ë³€í™˜ ì™„ë£Œ: {len(nodes_to_create)}ê°œì˜ ë…¸ë“œ, {len(relationships_to_create)}ê°œì˜ ê´€ê³„ ìƒì„±ë¨.")
    logger.info("--- Phase 2: Transform ì¢…ë£Œ ---")
    
    return nodes_to_create, relationships_to_create


def _unify_text_sources(mysql_data: MysqlData, pdf_texts: PdfTextData) -> Tuple[Dict[int, str], Dict[int, Any]]:
    """
    (Helper) ê° í”¼ë“œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì†ŒìŠ¤(title, summary, original_text, pdf_text)ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨.
    - NLP ë¶„ì„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ë‹¨ì¼ 'ë¬¸ì„œ'ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì„.
    - ë°˜í™˜ê°’: ( {feed_id: "ì „ì²´ í…ìŠ¤íŠ¸"}, {feed_id: feed_ê°ì²´} )
    """
    logger.info("  - unifying text sources for each feed...")
    
    # feed_idë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ í”¼ë“œ ê°ì²´ì— ë¹ ë¥´ê²Œ ì ‘ê·¼í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    feed_map = {feed['id']: feed for feed in mysql_data.get('feeds', [])}
    
    # {feed_id: "í†µí•©ëœ ì „ì²´ í…ìŠ¤íŠ¸"} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    all_feed_texts = {}

    for feed_id, feed in feed_map.items():
        # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸: ì œëª©(title)ê³¼ ìš”ì•½ë¬¸(summary)ì€ í•­ìƒ í¬í•¨
        #    - None ê°’ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´('')ë¡œ ì²˜ë¦¬
        title = feed.get('title', '') or ''
        summary = feed.get('summary', '') or ''
        
        # ê° í…ìŠ¤íŠ¸ ìš”ì†Œë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ëª…í™•í•˜ê²Œ ë¶„ë¦¬í•˜ì—¬ ê²°í•©
        full_text_parts = [title, summary]

        # 2. ì½˜í…ì¸  íƒ€ì…ì— ë”°ë¼ ì›ë¬¸(ë³¸ë¬¸) ì¶”ê°€
        content_type = feed.get('content_type')
        
        if content_type == 'text':
            # content_typeì´ 'text'ì¸ ê²½ìš°, original_text ì»¬ëŸ¼ì˜ ê°’ì„ ì¶”ê°€
            original_text = feed.get('original_text', '') or ''
            full_text_parts.append(original_text)
            
        elif content_type == 'pdf':
            # content_typeì´ 'pdf'ì¸ ê²½ìš°, Extract ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•œ PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
            # pdf_texts ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ feed_idì˜ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ì˜´
            pdf_content = pdf_texts.get(feed_id, '') or ''
            full_text_parts.append(pdf_content)
        
        # 3. ëª¨ë“  í…ìŠ¤íŠ¸ ì¡°ê°ì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©
        all_feed_texts[feed_id] = "\n".join(filter(None, full_text_parts))

    return all_feed_texts, feed_map


def _vectorize_texts(texts: List[str]) -> Tuple[Any, TfidfVectorizer]:
    """
    (Helper) í†µí•©ëœ í…ìŠ¤íŠ¸ ëª¨ìŒì„ TF-IDF í–‰ë ¬ë¡œ ë³€í™˜í•¨.
    - ë‚´ë¶€ì ìœ¼ë¡œ kiwi_tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•¨.
    - ë°˜í™˜ê°’: (TF-IDF í–‰ë ¬, í•™ìŠµëœ Vectorizer ê°ì²´)
    """
    logger.info("  - TF-IDF Vectorizer ìƒì„± ë° í•™ìŠµ ì‹œì‘...")

    # TfidfVectorizer ê°ì²´ ìƒì„±.
    # ì´ ê°ì²´ê°€ NLPì˜ í•µì‹¬ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•¨.
    vectorizer = TfidfVectorizer(
        # tokenizer: í…ìŠ¤íŠ¸ë¥¼ ì–´ë–¤ ë‹¨ìœ„(í† í°)ë¡œ ìª¼ê°¤ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜.
        #           ìš°ë¦¬ê°€ ë§Œë“  kiwi_tokenizerë¥¼ ì§€ì •í•˜ì—¬ í•œêµ­ì–´ ëª…ì‚¬ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ê²Œ í•¨.
        tokenizer=kiwi_tokenizer,
        
        # max_df: "Document Frequency"ì˜ ìµœëŒ€ê°’ (0.0 ~ 1.0 ì‚¬ì´).
        #         ë„ˆë¬´ ë§ì€ ë¬¸ì„œ(ì˜ˆ: ì „ì²´ì˜ 85% ì´ìƒ)ì— ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ”
        #         ë¶„ì„ì—ì„œ ì œì™¸í•¨. 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ' ë“±ê³¼ ê°™ì€ ë¶ˆìš©ì–´(stopword)ì¼
        #         ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì„.
        max_df=0.85,
        
        # min_df: "Document Frequency"ì˜ ìµœì†Œê°’ (ì •ìˆ˜).
        #         ë„ˆë¬´ ì ì€ ìˆ˜ì˜ ë¬¸ì„œ(ì˜ˆ: 2ê°œ ë¯¸ë§Œ)ì—ë§Œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ”
        #         ë¬´ì‹œí•¨. ì˜¤íƒˆìì´ê±°ë‚˜ ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ì¼
        #         ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì„.
        min_df=2,

        # ngram_range: í•¨ê»˜ ê³ ë ¤í•  ë‹¨ì–´ì˜ ë²”ìœ„. (min, max) íŠœí”Œ.
        #              (1, 2)ëŠ” 'ë¶€ë™ì‚°' ê°™ì€ ë‹¨ì¼ ë‹¨ì–´(1-gram) ë¿ë§Œ ì•„ë‹ˆë¼,
        #              'ë¶€ë™ì‚° ì •ì±…' ê°™ì€ ì—°ì†ëœ ë‘ ë‹¨ì–´(2-gram)ë„ í•˜ë‚˜ì˜
        #              í‚¤ì›Œë“œë¡œ í•¨ê»˜ ê³ ë ¤í•¨. í‚¤ì›Œë“œì˜ ì˜ë¯¸ë¥¼ í›¨ì”¬ í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ ì¤Œ.
        ngram_range=(1, 2)
    )

    # .fit_transform(): í…ìŠ¤íŠ¸ ë°ì´í„°ì— ë²¡í„°ë¼ì´ì €ë¥¼ í•™ìŠµ(fit)ì‹œí‚¤ê³ ,
    #                  ê·¸ ê²°ê³¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ TF-IDF í–‰ë ¬ë¡œ ë³€í™˜(transform)í•¨.
    # ì´ ê³¼ì •ì´ ê°€ì¥ ë§ì€ ì—°ì‚°ëŸ‰ì„ ìš”êµ¬í•˜ëŠ” ë¶€ë¶„ì„.
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    logger.info(f"  - TF-IDF í–‰ë ¬ ìƒì„± ì™„ë£Œ. (í¬ê¸°: {tfidf_matrix.shape})")

    # ë‹¤ìŒ ë‹¨ê³„(ìœ ì‚¬ë„ ê³„ì‚°, í‚¤ì›Œë“œ ì¶”ì¶œ)ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´
    # ë³€í™˜ëœ í–‰ë ¬ê³¼ í•™ìŠµì´ ì™„ë£Œëœ ë²¡í„°ë¼ì´ì € ê°ì²´ë¥¼ ëª¨ë‘ ë°˜í™˜í•¨.
    return tfidf_matrix, vectorizer


def _get_top_keywords(tfidf_vector, vectorizer, top_n=10) -> List[Tuple[str, float]]:
    """(Sub-Helper) íŠ¹ì • ë¬¸ì„œì˜ TF-IDF ë²¡í„°ì—ì„œ ìƒìœ„ Nê°œì˜ í‚¤ì›Œë“œì™€ ì ìˆ˜ë¥¼ ì¶”ì¶œí•¨."""
    # ë²¡í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
    flat_vector = tfidf_vector.toarray().flatten()
    # TF-IDF ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬
    top_indices = np.argsort(flat_vector)[-top_n:][::-1]
    
    feature_names = vectorizer.get_feature_names_out()
    
    keywords = []
    for i in top_indices:
        # ì ìˆ˜ê°€ 0ì¸ ê²½ìš°ëŠ” í‚¤ì›Œë“œë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        if flat_vector[i] > 0:
            keywords.append((feature_names[i], round(float(flat_vector[i]), 4)))
            
    return keywords


def _structure_graph_data(
    mysql_data: MysqlData, 
    search_logs: SearchLogData,
    feed_map: Dict[int, Any],
    vectorizer: TfidfVectorizer, 
    tfidf_matrix, #ç¨€ç–çŸ©é˜µ
    similarity_matrix, #å¯†é›†çŸ©é˜µ
    similarity_threshold: float = 0.2 # ìœ ì‚¬ë„ ì„ê³„ê°’
) -> TransformedData:
    """
    (Helper) ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ Neo4jì— ì ì¬í•  ìµœì¢… í˜•íƒœë¡œ êµ¬ì¡°í™”í•¨.
    - ì´ í•¨ìˆ˜ëŠ” ETLì˜ 'Transform' ë‹¨ê³„ì˜ ìµœì¢… ì¡°ë¦½ ë¼ì¸ì„.
    """
    nodes = []
    relationships = []

    # --- 1. MySQL ë°ì´í„° ê¸°ë°˜ ë…¸ë“œ ë° ê´€ê³„ ìƒì„± ---
    logger.info("    - 1/4: MySQL ë°ì´í„° ê¸°ë°˜ ë…¸ë“œ/ê´€ê³„ êµ¬ì¡°í™”...")
    nodes.extend([{'label': 'User', **user} for user in mysql_data['users']])
    nodes.extend([{'label': 'Organization', **org} for org in mysql_data['organizations']])
    nodes.extend([{'label': 'Category', **cat} for cat in mysql_data['categories']])
    nodes.extend([{'label': 'Feed', **feed} for feed in mysql_data['feeds']])

    # RATED ê´€ê³„ (ì ìˆ˜ë³„ë¡œ ì„¸ë¶„í™”)
    for rating in mysql_data['ratings']:
        score = rating['score']
        if score >= 4: rel_type = 'RATED_POSITIVELY'
        elif score == 3: rel_type = 'RATED_NORMALLY'
        else: rel_type = 'RATED_NEGATIVELY'
        relationships.append({
            'start_node': ('User', rating['user_id']),
            'end_node': ('Feed', rating['feed_id']),
            'type': rel_type,
            'properties': {'score': score}
        })
    # ê¸°íƒ€ MySQL ê¸°ë°˜ ê´€ê³„
    relationships.extend([
        {'start_node': ('User', bm['user_id']), 'end_node': ('Feed', bm['feed_id']), 'type': 'BOOKMARKED'}
        for bm in mysql_data['bookmarks']
    ])
    relationships.extend([
        {'start_node': ('Organization', feed['organization_id']), 'end_node': ('Feed', feed['id']), 'type': 'PUBLISHED'}
        for feed in mysql_data['feeds']
    ])
    relationships.extend([
        {'start_node': ('Feed', feed['id']), 'end_node': ('Category', feed['category_id']), 'type': 'BELONGS_TO'}
        for feed in mysql_data['feeds']
    ])

    # --- 2. NLP ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ë…¸ë“œ ë° ê´€ê³„ ìƒì„± ---
    logger.info("    - 2/4: NLP ê¸°ë°˜ í‚¤ì›Œë“œ ë…¸ë“œ/ê´€ê³„ êµ¬ì¡°í™”...")
    # vectorizerì˜ ë‹¨ì–´ ì‚¬ì „ ìì²´ê°€ Keyword ë…¸ë“œì˜ í›„ë³´ê°€ ë¨
    all_keywords = vectorizer.get_feature_names_out()
    nodes.extend([{'label': 'Keyword', 'id': keyword, 'name': keyword} for keyword in all_keywords])
    
    feed_ids = list(feed_map.keys())
    for i, feed_id in enumerate(feed_ids):
        # ê° í”¼ë“œ(ë¬¸ì„œ)ì˜ TF-IDF ë²¡í„°ì—ì„œ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        top_keywords = _get_top_keywords(tfidf_matrix[i], vectorizer, top_n=10)
        for keyword, score in top_keywords:
            relationships.append({
                'start_node': ('Feed', feed_id),
                'end_node': ('Keyword', keyword),
                'type': 'CONTAINS_KEYWORD',
                'properties': {'score': score}
            })

    # --- 3. NLP ë¶„ì„ ê¸°ë°˜ ìœ ì‚¬ë„ ê´€ê³„ ìƒì„± ---
    logger.info("    - 3/4: NLP ê¸°ë°˜ ìœ ì‚¬ë„ ê´€ê³„ êµ¬ì¡°í™”...")
    # similarity_matrixëŠ” ìê¸° ìì‹ ê³¼ì˜ ë¹„êµë„ í¬í•¨í•˜ë¯€ë¡œ, ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ ìƒë‹¨ ì‚¼ê°í˜• ë¶€ë¶„ë§Œ ìˆœíšŒ
    for i in range(len(feed_ids)):
        for j in range(i + 1, len(feed_ids)):
            score = similarity_matrix[i, j]
            if score >= similarity_threshold:
                relationships.append({
                    'start_node': ('Feed', feed_ids[i]),
                    'end_node': ('Feed', feed_ids[j]),
                    'type': 'IS_SIMILAR_TO',
                    'properties': {'score': round(float(score), 4)}
                })

    # --- 4. Elasticsearch ë¡œê·¸ ê¸°ë°˜ ê²€ìƒ‰ ê´€ê³„ ìƒì„± ---
    logger.info("    - 4/4: ê²€ìƒ‰ ë¡œê·¸ ê¸°ë°˜ ê´€ê³„ êµ¬ì¡°í™”...")
    for user_id, keyword in search_logs:
        # ê²€ìƒ‰ëœ í‚¤ì›Œë“œê°€ ìš°ë¦¬ ë‹¨ì–´ ì‚¬ì „ì— ìˆëŠ” ê²½ìš°ì—ë§Œ ê´€ê³„ë¥¼ ìƒì„±
        if keyword in all_keywords:
            start_node_label = 'User' if user_id != 'anonymous' else 'AnonymousUser'
            # (AnonymousUser ë…¸ë“œë¥¼ ìœ„í•œ ì²˜ë¦¬ - ì§€ê¸ˆì€ Userì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ë˜, IDë§Œ ë‹¤ë¦„)
            if start_node_label == 'AnonymousUser':
                # ìµëª… ì‚¬ìš©ì ë…¸ë“œê°€ ì—†ë‹¤ë©´ ì¶”ê°€ (ë‹¨ í•œë²ˆë§Œ)
                if not any(n['label'] == 'AnonymousUser' for n in nodes):
                    nodes.append({'label': 'AnonymousUser', 'id': 'anonymous', 'name': 'Anonymous User'})
            
            relationships.append({
                'start_node': (start_node_label, user_id),
                'end_node': ('Keyword', keyword),
                'type': 'SEARCHED'
            })

    return nodes, relationships


# =================================== LOAD ===================================
def phase_load():
    logger.info("--- Phase 3: Load ì‹œì‘ ---")
    pass

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©) ---
async def run_pipeline_for_dev():
    """
    ê°œë°œ í™˜ê²½ì—ì„œ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜.
    - ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ í˜¸ì¶œí•  ê²ƒì„.
    """
    logger.info("======= Knowledge Graph ETL Pipeline (DEV) ì‹œì‘ =======")
    
    async with AsyncSessionLocal() as db:
        try:
            # 1. Extract
            mysql_data, pdf_texts, search_logs = await phase_extract(db)
            
            # 2. Transform (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)
            # transformed_data = phase_transform(mysql_data, pdf_texts, search_logs)
            
            # 3. Load (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)
            # await phase_load(db, transformed_data)

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    logger.info("======= Knowledge Graph ETL Pipeline (DEV) ì¢…ë£Œ =======")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(run_pipeline_for_dev())