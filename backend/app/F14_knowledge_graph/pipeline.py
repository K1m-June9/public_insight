import asyncio
import logging
import os
import fitz
from typing import Dict, List, Any, Tuple

# --- SQLAlchemy ë¹„ë™ê¸° ì„¤ì • ---
from sqlalchemy import select
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# --- í”„ë¡œì íŠ¸ ëª¨ë¸ ì„í¬íŠ¸ ---
# (settingsì™€ ëª¨ë¸ ê²½ë¡œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
from app.F5_core.config import settings
from app.F7_models.users import User
from app.F7_models.organizations import Organization
from app.F7_models.categories import Category
from app.F7_models.feeds import Feed
from app.F7_models.bookmarks import Bookmark
from app.F7_models.ratings import Rating

# --- ë¡œê¹… ì„¤ì • ---
logger = logging.getLogger(__name__)


# --- ë°ì´í„° êµ¬ì¡° ì •ì˜ (Type Hinting) ---
MysqlData = Dict[str, List[Dict[str, Any]]]
PdfTextData = Dict[int, str]
SearchLogData = List[Tuple[str, str]]


# ---------- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ í•­ìƒ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì°¾ë„ë¡ ì´ˆê¸°ì— ì„¤ì • ----------
# 1. í˜„ì¬ ì´ íŒŒì¼(pipeline.py)ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì°¾ìŒ
#    ex: /home/pumpkinbee/public_insight/app/F14_knowledge_graph/pipeline.py
current_file_path = os.path.abspath(__file__)

# 2. F14_knowledge_graph í´ë”ì˜ ê²½ë¡œë¥¼ ì°¾ìŒ (í•œ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight/app/F14_knowledge_graph
f14_dir = os.path.dirname(current_file_path)

# 3. app í´ë”ì˜ ê²½ë¡œë¥¼ ì°¾ìŒ (ë‘ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight/app
app_dir = os.path.dirname(f14_dir)

# 4. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ ì°¾ìŒ (ì„¸ ë‹¨ê³„ ìœ„)
#    ex: /home/pumpkinbee/public_insight
project_root_dir = os.path.dirname(app_dir)

# 5. ë£¨íŠ¸ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ static í´ë”ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒì„±í•¨
#    ì´ë ‡ê²Œ í•˜ë©´ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  í•­ìƒ ë™ì¼í•œ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ë¦¬í‚´
PDF_BASE_PATH = os.path.join(project_root_dir, "backend", "static", "feeds_pdf")

# --- MySQL ì—°ê²° ì„¤ì • (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©) ---
DATABASE_URL = (
    f"mysql+aiomysql://{settings.DB_USER}:{settings.DB_PASSWORD}@"
    f"{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"
)
engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)


async def phase_extract(db: AsyncSession) -> Tuple[MysqlData, PdfTextData, SearchLogData]:
    """
    ETL íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„: Extract
    - ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤(MySQL, PDF, Elasticsearch)ì—ì„œ ì›ë³¸ ë°ì´í„° ì¶”ì¶œí•¨.
    - ë°ì´í„° ê°€ê³µ ì—†ì´, ìˆëŠ” ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì— ì§‘ì¤‘í•¨.
    """
    logger.info("--- Phase 1: Extract ì‹œì‘ ---")

    # 1.1: MySQLì—ì„œ ë°ì´í„° ì¶”ì¶œ
    mysql_data = await _extract_from_mysql(db)
    feed_count = len(mysql_data.get('feeds', []))
    logger.info(f"MySQLì—ì„œ {feed_count}ê°œì˜ í”¼ë“œ í¬í•¨, ì „ì²´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ.")

    # 1.2: PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # ğŸ”§ ìˆ˜ì •: content_typeì´ 'PDF'ì¸ í”¼ë“œë§Œ í•„í„°ë§í•˜ì—¬ ì „ë‹¬í•¨
    feeds_with_pdf = [
        f for f in mysql_data.get('feeds', []) 
        if f.get('content_type') == 'pdf' and f.get('pdf_file_path')
    ]
    pdf_texts = _extract_text_from_pdfs(feeds_with_pdf)
    logger.info(f"{len(pdf_texts)}ê°œì˜ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
    
    # 1.3: Elasticsearchì—ì„œ ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ
    search_logs = _extract_search_logs_from_es()
    logger.info(f"Elasticsearchì—ì„œ {len(search_logs)}ê°œì˜ ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ ì™„ë£Œ.")

    logger.info("--- Phase 1: Extract ì¢…ë£Œ ---")
    
    return mysql_data, pdf_texts, search_logs


async def _extract_from_mysql(db: AsyncSession) -> MysqlData:
    """
    (Helper) SQLAlchemyë¥¼ ì‚¬ìš©í•˜ì—¬ MySQLì˜ 6ê°œ í•µì‹¬ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´.
    - Feed í…Œì´ë¸”ì—ì„œ content_typeê³¼ original_textë¥¼ ì¶”ê°€ë¡œ ì¡°íšŒí•¨.
    """
    user_res = await db.execute(select(User.id, User.user_id, User.nickname))
    org_res = await db.execute(select(Organization.id, Organization.name))
    cat_res = await db.execute(select(Category.id, Category.name, Category.organization_id))
    
    # ğŸ”§ ìˆ˜ì •: Feed í…Œì´ë¸” ì¡°íšŒ ì‹œ content_typeê³¼ original_text ì»¬ëŸ¼ ì¶”ê°€
    feed_res = await db.execute(select(
        Feed.id, Feed.title, Feed.summary, 
        Feed.content_type, Feed.pdf_file_path, Feed.original_text,
        Feed.organization_id, Feed.category_id, Feed.published_date
    ))
    
    bookmark_res = await db.execute(select(Bookmark.user_id, Bookmark.feed_id))
    rating_res = await db.execute(select(Rating.user_id, Rating.feed_id, Rating.score))

    data = {
        "users": [dict(r) for r in user_res.mappings().all()],
        "organizations": [dict(r) for r in org_res.mappings().all()],
        "categories": [dict(r) for r in cat_res.mappings().all()],
        "feeds": [dict(r) for r in feed_res.mappings().all()],
        "bookmarks": [dict(r) for r in bookmark_res.mappings().all()],
        "ratings": [dict(r) for r in rating_res.mappings().all()],
    }
    return data


def _extract_text_from_pdfs(feeds: List[Dict[str, Any]]) -> PdfTextData:
    """
    (Helper) PyMuPDF (fitz)ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ëª©ë¡ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•¨.
    - ê° í”¼ë“œì˜ 'pdf_file_path'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ êµ¬ì„±í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì½ìŒ.
    - íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ, í•´ë‹¹ í”¼ë“œëŠ” ê±´ë„ˆë›°ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹€.
    """
    # ğŸ’¥ ì¤‘ìš”: ì´ ê²½ë¡œëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ í•¨.
    # ë³´í†µ í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì„.
    base_path = PDF_BASE_PATH
    extracted_texts = {}

    for feed in feeds:
        feed_id = feed.get('id')
        relative_path = feed.get('pdf_file_path')

        if not feed_id or not relative_path:
            continue

        # UUID íŒŒì¼ëª…ì— .pdf í™•ì¥ìë¥¼ ì¶”ê°€í•˜ì—¬ ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±
        pdf_path = os.path.join(base_path, f"{relative_path}.pdf")

        try:
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(pdf_path):
                logger.warning(f"PDF íŒŒì¼ ì—†ìŒ (ê±´ë„ˆëœ€): {pdf_path}")
                continue
            
            # PDF íŒŒì¼ì„ ì—´ê³  ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²°í•©í•¨
            doc = fitz.open(pdf_path)
            full_text = ""
            for page in doc:
                full_text += page.get_text()
            
            extracted_texts[feed_id] = full_text
            doc.close()

        except Exception as e:
            # PyMuPDF ì²˜ë¦¬ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•¨
            logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {pdf_path} | ì˜¤ë¥˜: {e}")
            continue
            
    return extracted_texts


def _extract_search_logs_from_es() -> SearchLogData:
    """
    (Helper) Elasticsearch Python í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê·¼ ê²€ìƒ‰ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜´.
    """
    # TODO: Elasticsearch ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ë¥¼ ì¡°íšŒí•˜ëŠ” ë¡œì§ êµ¬í˜„
    logger.info("TODO: Elasticsearch ê²€ìƒ‰ ë¡œê·¸ ì¶”ì¶œ ë¡œì§ êµ¬í˜„")
    # ì„ì‹œ ë°ì´í„°
    return [("1", "ì²­ë…„ì£¼íƒ"), ("anonymous", "ë¶€ë™ì‚° ì •ì±…")]

# --- ë‹¤ìŒ ë‹¨ê³„ í•¨ìˆ˜ (ë¼ˆëŒ€) ---
def phase_transform(mysql_data: MysqlData, pdf_texts: PdfTextData, search_logs: SearchLogData):
    logger.info("--- Phase 2: Transform ì‹œì‘ ---")
    pass

def phase_load():
    logger.info("--- Phase 3: Load ì‹œì‘ ---")
    pass

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©) ---
async def run_pipeline_for_dev():
    """
    ê°œë°œ í™˜ê²½ì—ì„œ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜.
    - ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ í˜¸ì¶œí•  ê²ƒì„.
    """
    logger.info("======= Knowledge Graph ETL Pipeline (DEV) ì‹œì‘ =======")
    
    async with AsyncSessionLocal() as db:
        try:
            # 1. Extract
            mysql_data, pdf_texts, search_logs = await phase_extract(db)
            
            # 2. Transform (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)
            # transformed_data = phase_transform(mysql_data, pdf_texts, search_logs)
            
            # 3. Load (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)
            # await phase_load(db, transformed_data)

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    logger.info("======= Knowledge Graph ETL Pipeline (DEV) ì¢…ë£Œ =======")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(run_pipeline_for_dev())