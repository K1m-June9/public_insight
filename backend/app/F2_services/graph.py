import logging
import math
from typing import Union, Dict, Any, List

from app.F3_repositories.graph import GraphRepository
# ğŸ”§ [ìˆ˜ì •] ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ ìŠ¤í‚¤ë§ˆë“¤ì„ ì„í¬íŠ¸
from app.F6_schemas.graph import (
    ExploreGraphResponse, 
    ExploreGraphData, 
    GraphNode, 
    GraphEdge,
    WordCloudItem,
    WordCloudResponse,
    RelatedKeywordItem,
    RelatedKeywordsResponse
)
from app.F6_schemas.base import ErrorResponse, ErrorDetail, ErrorCode, Message

logger = logging.getLogger(__name__)

class GraphService:
    """
    ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ì™€ ê´€ë ¨ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤.
    - ë¦¬í¬ì§€í† ë¦¬ë¡œë¶€í„° ë°›ì€ ë°ì´í„°ë¥¼ API ì‘ë‹µ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ê°€ê³µí•˜ê³  ë³€í™˜í•¨.
    """
    def __init__(self, repo: GraphRepository):
        self.repo = repo

    async def get_initial_graph_by_keyword(
        self, keyword: str
    ) -> Union[ExploreGraphResponse, ErrorResponse]:
        """
        í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì¸ë“œë§µ ì´ˆê¸° ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  êµ¬ì¡°í™”í•¨.
        """
        try:
            # 1. ë¦¬í¬ì§€í† ë¦¬ë¥¼ í˜¸ì¶œí•˜ì—¬ Neo4jë¡œë¶€í„° ì›ì‹œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            raw_graph_data = await self.repo.find_initial_nodes_by_keyword(keyword)

            # 2. ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (ë¦¬í¬ì§€í† ë¦¬ê°€ Noneì„ ë°˜í™˜í•œ ê²½ìš°)
            if not raw_graph_data:
                return ErrorResponse(
                    error=ErrorDetail(
                        code=ErrorCode.NOT_FOUND,
                        message=f"í‚¤ì›Œë“œ '{keyword}'ì™€ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
                )

            # 3. ì›ì‹œ ë°ì´í„°ë¥¼ nodesì™€ edges ë¦¬ìŠ¤íŠ¸ë¡œ 'ì¬ì¡°ë¦½'
            nodes, edges = self._structure_for_frontend(raw_graph_data)

            # 4. ìµœì¢… ì„±ê³µ ì‘ë‹µ ìŠ¤í‚¤ë§ˆì— ë°ì´í„°ë¥¼ ë‹´ì•„ ìƒì„±
            response_data = ExploreGraphData(nodes=nodes, edges=edges)
            
            return ExploreGraphResponse(success=True, data=response_data)

        except Exception as e:
            logger.error(f"Error in GraphService for keyword '{keyword}': {e}", exc_info=True)
            return ErrorResponse(
                error=ErrorDetail(
                    code=ErrorCode.INTERNAL_ERROR,
                    message=Message.INTERNAL_ERROR
                )
            )
            
    def _structure_for_frontend(self, raw_data: dict) -> tuple[list[GraphNode], list[GraphEdge]]:
        """
        (Helper) ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë°›ì€ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜í•¨.
        - ë…¸ë“œì˜ ì´ˆê¸° ìœ„ì¹˜(x, y)ì™€ ë¶€ê°€ ì •ë³´(ë©”íƒ€ë°ì´í„°)ë¥¼ ê³„ì‚°í•˜ì—¬ í¬í•¨í•¨.
        - ì´ˆê¸° í™”ë©´ì—ì„œëŠ” ì¤‘ì‹¬ í‚¤ì›Œë“œì™€ 1ì°¨ ì—°ê²°ëœ ë…¸ë“œ/ì—£ì§€ë§Œ ìƒì„±í•¨.
        """
        nodes: List[GraphNode] = []
        edges: List[GraphEdge] = []
        
        # --- 1. ì¤‘ì‹¬ í‚¤ì›Œë“œ ë…¸ë“œ ìƒì„± ---
        keyword_node_data = raw_data.get('keyword')
        if not keyword_node_data:
            return [], []
            
        keyword_id_str = f"keyword_{keyword_node_data['name']}"
        
        keyword_node = GraphNode(
            id=keyword_id_str,
            type='keyword',
            label=keyword_node_data['name'],
            metadata={} # ì¢Œí‘œëŠ” ë‚˜ì¤‘ì— í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ê³„ì‚°í•˜ë¯€ë¡œ ë¹„ì›Œë‘ 
        )
        nodes.append(keyword_node)

        # --- 2. ê´€ë ¨ í”¼ë“œ ë° ê¸°ê´€ ë…¸ë“œ ìƒì„± ---
        # í”¼ë“œì™€ ê¸°ê´€ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ë‘ë©´ ì¡°íšŒê°€ ë¹ ë¦„
        feeds_map = {feed['id']: feed for feed in raw_data.get('feeds', [])}
        orgs_map = {org['id']: org for org in raw_data.get('organizations', [])}

        # í”¼ë“œ ë…¸ë“œ ìƒì„±
        for feed_id, feed_data in feeds_map.items():
            nodes.append(GraphNode(
                id=f"feed_{feed_id}",
                type='feed',
                label=feed_data['title'],
                metadata={
                    # ìš°ë¦¬ê°€ ë…¼ì˜í–ˆë˜ ëª¨ë“  ë¶€ê°€ ì •ë³´ë¥¼ metadataì— ì¶”ê°€
                    'published_date': str(feed_data.get('published_date')),
                    'view_count': feed_data.get('view_count'),
                    'avg_rating': feed_data.get('average_rating'),
                    'bookmark_count': feed_data.get('bookmark_count')
                }
            ))
        
        # ê¸°ê´€ ë…¸ë“œ ìƒì„±
        for org_id, org_data in orgs_map.items():
            nodes.append(GraphNode(
                id=f"organization_{org_id}",
                type='organization',
                label=org_data['name'],
                metadata={}
            ))
            
        # --- 3. 1ë‹¨ê³„ ì—£ì§€(ê´€ê³„) ìƒì„± ---
        # ë…¸íŠ¸ë¶LM ìŠ¤íƒ€ì¼ì„ ìœ„í•´, ì´ˆê¸° í™”ë©´ì—ì„œëŠ” ì¤‘ì‹¬ í‚¤ì›Œë“œì™€ ë‹¤ë¥¸ ë…¸ë“œë“¤ì„ ì‡ëŠ” ì—£ì§€ë§Œ ìƒì„±í•¨.
        for node in nodes:
            # ğŸ”§ [ìˆ˜ì •] ê°ì²´ì˜ ì†ì„±ì— ì ‘ê·¼í•  ë•ŒëŠ” '.'ì„ ì‚¬ìš©
            # ìê¸° ìì‹ ì´ ì•„ë‹ˆê³ , ì¤‘ì‹¬ ë…¸ë“œ(keyword_node)ê°€ ì•„ë‹Œ ë…¸ë“œë“¤ë§Œ ì—°ê²°
            if node.id != keyword_node.id:
                edges.append(GraphEdge(
                    id=f"{keyword_node.id}-INITIAL_LINK-{node.id}",
                    source=keyword_node.id, # ëª¨ë“  ì—£ì§€ëŠ” ì¤‘ì‹¬ í‚¤ì›Œë“œì—ì„œ ì‹œì‘
                    target=node.id,
                ))
                
        return nodes, edges
        
    async def get_expanded_graph_by_node(
        self, node_id: str, node_type: str,
        exclude_ids_str: str | None = None
    ) -> Union[ExploreGraphResponse, ErrorResponse]:
        """
        [ML ê¸°ë°˜ìœ¼ë¡œ ë¦¬íŒ©í† ë§ë¨]
        í´ë¦­ëœ ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ í™•ì¥í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì— ë”°ë¼ ê²°ê³¼ë¥¼ í•„í„°ë§í•¨.
        """
        try:
            entity_id = node_id.split('_', 1)[-1]
            exclude_ids = set(exclude_ids_str.split(',')) if exclude_ids_str else set()

            raw_expansion_data: Dict[str, Any] | None = None
            rules: Dict[str, int] = {}

            if node_type == 'feed':
                raw_expansion_data = await self.repo.expand_from_feed(int(entity_id), exclude_ids)
                rules = {'feed': 4, 'keyword': 3, 'organization': 1}
            
            elif node_type == 'organization':
                raw_expansion_data = await self.repo.expand_from_organization(int(entity_id), exclude_ids)
                rules = {'feed': 4, 'keyword': 3}
            
            elif node_type == 'keyword':
                raw_expansion_data = await self.repo.expand_from_keyword(str(entity_id), exclude_ids)
                rules = {'feed': 4, 'keyword': 2, 'organization': 1}
            
            else:
                return ErrorResponse(error=ErrorDetail(code=ErrorCode.BAD_REQUEST, message="ì§€ì›í•˜ì§€ ì•ŠëŠ” ë…¸ë“œ íƒ€ì…ì…ë‹ˆë‹¤."))

            if not raw_expansion_data:
                return ExploreGraphResponse(success=True, data=ExploreGraphData(nodes=[], edges=[]))
            
            final_node_infos = self._filter_and_select_nodes(raw_expansion_data, rules, exclude_ids)
            
            # [í•µì‹¬ ìˆ˜ì • 1] ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ì˜¨ 'predicted_details'ë¥¼ ë³€ìˆ˜ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            predicted_details = raw_expansion_data.get("predicted_details", {})
            
            # [í•µì‹¬ ìˆ˜ì • 2] ì¶”ì¶œí•œ 'predicted_details'ë¥¼ í—¬í¼ í•¨ìˆ˜ì— ì¸ìë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            nodes, edges = self._structure_expansion_for_frontend(
                node_id, final_node_infos, predicted_details
            )
            
            response_data = ExploreGraphData(nodes=nodes, edges=edges)
            return ExploreGraphResponse(success=True, data=response_data)

        except Exception as e:
            logger.error(f"Error expanding graph for node '{node_id}': {e}", exc_info=True)
            return ErrorResponse(error=ErrorDetail(code=ErrorCode.INTERNAL_ERROR, message=Message.INTERNAL_ERROR))

    def _filter_and_select_nodes(
        self, 
        raw_data: Dict[str, Any], 
        rules: Dict[str, int],
        exclude_ids: set[str]
    ) -> List[Dict[str, Any]]:
        """
        (Helper) ML ì˜ˆì¸¡ ê²°ê³¼ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì— ë”°ë¼ ìµœì¢… ë…¸ë“œë¥¼ ì„ ë³„í•¨.
        - [ìˆ˜ì •] ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” ë…¸ë“œê°€ ì˜ˆì¸¡ë§Œ ìˆëŠ” ë…¸ë“œë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆë„ë¡ ë¡œì§ ë³€ê²½
        """
        selected_nodes_map: Dict[str, Dict[str, Any]] = {}
        counts = {node_type: 0 for node_type in rules.keys()}
        
        # 1. ì˜ˆì¸¡ëœ ë…¸ë“œë“¤ë¶€í„° ê·œì¹™ì— ë”°ë¼ 'ìë¦¬'ë¥¼ ì±„ì›Œë„£ê¸° (ìƒì„¸ ì •ë³´ëŠ” ì•„ì§ ì—†ìŒ)
        for node_id, similarity in raw_data.get("predicted_nodes", []):
            node_type = node_id.split('_')[0]
            
            if node_type in rules and counts[node_type] < rules[node_type] and node_id not in exclude_ids:
                selected_nodes_map[node_id] = {'id': node_id, 'type': node_type, 'similarity': similarity}
                counts[node_type] += 1

        # 2. Cypherë¡œ í™•ì •ì ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë…¸ë“œ(ê¸°ê´€ ë“±)ë¡œ 'ì •ë³´ë¥¼ ë³´ê°•'í•˜ê±°ë‚˜ 'ìƒˆë¡œ ì¶”ê°€'
        explicit_nodes = raw_data.get("explicit_nodes", {})
        for node_type, node_or_list in explicit_nodes.items():
            node_type_singular = node_type.rstrip('s') 
            
            if node_type_singular in rules:
                nodes_to_process = node_or_list if isinstance(node_or_list, list) else [node_or_list]
                
                for node_data in nodes_to_process:
                    if not node_data: continue
                    
                    node_id = f"{node_type_singular}_{node_data['id']}"
                    
                    # [í•µì‹¬ ìˆ˜ì •] 
                    # ì´ë¯¸ ë§µì— ì˜ˆì¸¡ ë…¸ë“œ(ìë¦¬ë§Œ ìˆëŠ”)ê°€ ìˆë”ë¼ë„, ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” í˜„ì¬ ë…¸ë“œê°€
                    # ê·¸ ìë¦¬ë¥¼ ë®ì–´ì“°ë„ë¡ 'and node_id not in selected_nodes_map' ì¡°ê±´ì„ ì œê±°í•©ë‹ˆë‹¤.
                    # ë‹¨, í• ë‹¹ëŸ‰(counts)ê³¼ ì œì™¸ ëª©ë¡(exclude_ids) ì²´í¬ëŠ” ì—¬ì „íˆ ìœ ì§€í•©ë‹ˆë‹¤.
                    if counts[node_type_singular] < rules[node_type_singular] and node_id not in exclude_ids:
                        
                        # ë§Œì•½ ì´ ë…¸ë“œê°€ ì˜ˆì¸¡ ëª©ë¡ì— ì—†ë˜ ìƒˆë¡œìš´ ë…¸ë“œë¼ë©´, ì¹´ìš´íŠ¸ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
                        if node_id not in selected_nodes_map:
                            counts[node_type_singular] += 1
                        
                        # ì˜ˆì¸¡ë§Œ ìˆë˜ ë…¸ë“œë¥¼ ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” ë…¸ë“œë¡œ ë®ì–´ì“°ê±°ë‚˜, ìƒˆë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
                        selected_nodes_map[node_id] = {'id': node_id, 'type': node_type_singular, 'data': node_data}

        return list(selected_nodes_map.values())

    def _structure_expansion_for_frontend(
        self, start_node_id: str, final_node_infos: List[Dict[str, Any]],
        # [í•µì‹¬ ìˆ˜ì • 3] 'predicted_details'ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„° ì¶”ê°€
        details_map: Dict[str, Any]
    ) -> tuple[List[GraphNode], List[GraphEdge]]:
        """
        (Helper) ìµœì¢… ì„ ë³„ëœ ë…¸ë“œ ì •ë³´ ëª©ë¡ì„ í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜í•¨.
        """
        nodes = []
        edges = []
        
        for item in final_node_infos:
            node_id = item['id']
            generic_type = item['type']
            
            # [í•µì‹¬ ìˆ˜ì • 4]
            # 'item' ìì²´ì˜ 'data'ê°€ ì•„ë‹Œ, íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ 'details_map'ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
            # ì´ê²ƒì´ ëª¨ë“  ì˜ˆì¸¡ëœ ë…¸ë“œ(feed, keyword, organization)ì˜ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
            node_data_from_details = details_map.get(node_id)
            node_data_from_item = item.get('data')

            # ìƒì„¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ IDì—ì„œ ë¼ë²¨ì„ ìœ ì¶”í•©ë‹ˆë‹¤.
            # details_mapì— ìˆëŠ” ì •ë³´ê°€ ë” ìš°ì„ ìˆœìœ„ê°€ ë†’ìŠµë‹ˆë‹¤.
            node_data = node_data_from_details or node_data_from_item
            label = node_data.get('title', node_data.get('name')) if node_data else node_id.split('_', 1)[-1]
            
            nodes.append(GraphNode(
                id=node_id,
                type=generic_type,
                label=label,
                metadata={}
            ))

            edges.append(GraphEdge(
                id=f"{start_node_id}-EXPANDS_TO-{node_id}",
                source=start_node_id,
                target=node_id
            ))

        return nodes, edges
    
    # ìƒˆë¡œ ë§Œë“œëŠ”ê±°ê¸´í•œë° ì •ë§ ëˆˆë¬¼ì´ ë‚œë‹¤.
    async def get_wordcloud_data(
        self, organization_name: str | None, limit: int
    ) -> Union[WordCloudResponse, ErrorResponse]:
        """
        ì›Œë“œí´ë¼ìš°ë“œ ë˜ëŠ” ì¸ê¸° í‚¤ì›Œë“œ ëª©ë¡ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  êµ¬ì¡°í™”í•¨.
        """
        try:
            # 1. ë¦¬í¬ì§€í† ë¦¬ë¥¼ í˜¸ì¶œí•˜ì—¬ Neo4jë¡œë¶€í„° ì›ì‹œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            keywords_data = await self.repo.get_keywords_by_popularity(organization_name, limit)
            
            # 2. Pydantic ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ë°ì´í„°ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ê³  êµ¬ì¡°ë¥¼ ë§ì¶¤.
            #    ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(keywords_dataê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°)ì—ë„ 
            #    Pydanticì´ ì•Œì•„ì„œ ì²˜ë¦¬í•˜ì—¬ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ì„±ê³µ ì‘ë‹µì„ ë§Œë“¤ì–´ ì¤Œ.
            response_data = [WordCloudItem(**item) for item in keywords_data]
            
            # 3. ìµœì¢… ì„±ê³µ ì‘ë‹µ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜
            return WordCloudResponse(success=True, data=response_data)

        except Exception as e:
            # ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë°œìƒí•œ ì˜ˆì™¸ë¥¼ í¬í•¨í•œ ëª¨ë“  ì—ëŸ¬ë¥¼ ì²˜ë¦¬
            logger.error(f"Error in get_wordcloud_data service: {e}", exc_info=True)
            return ErrorResponse(
                error=ErrorDetail(
                    code=ErrorCode.INTERNAL_ERROR,
                    message=Message.INTERNAL_ERROR
                )
            )
        
    async def get_related_keywords_for_feed(
        self, feed_id: int, limit: int = 10
    ) -> Union[RelatedKeywordsResponse, ErrorResponse]:
        """
        íŠ¹ì • í”¼ë“œì™€ ì—°ê´€ëœ í‚¤ì›Œë“œ ëª©ë¡ì„ ML ëª¨ë¸ì„ í†µí•´ ì¡°íšŒí•˜ê³  êµ¬ì¡°í™”í•¨.
        """
        try:
            # 1. ë¦¬í¬ì§€í† ë¦¬ë¥¼ í˜¸ì¶œí•˜ì—¬ (í‚¤ì›Œë“œ, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
            similar_keywords = await self.repo.get_similar_keywords_for_feed(feed_id, limit)
            
            # 2. [í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§]
            #    - ê° í‚¤ì›Œë“œì˜ ìœ ì‚¬ë„(0.0 ~ 1.0)ë¥¼ ì—°ê´€ë„ ì ìˆ˜(0 ~ 100)ë¡œ ë³€í™˜
            #    - Pydantic ìŠ¤í‚¤ë§ˆ(RelatedKeywordItem) ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ì¬ì¡°ë¦½
            response_data = [
                RelatedKeywordItem(
                    text=keyword,
                    score=int(similarity * 100) # ğŸ‘ˆ 0.98 -> 98ì ìœ¼ë¡œ ë³€í™˜
                ) 
                for keyword, similarity in similar_keywords
            ]
            
            # 3. ìµœì¢… ì„±ê³µ ì‘ë‹µ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜
            return RelatedKeywordsResponse(success=True, data=response_data)

        except Exception as e:
            # ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë°œìƒí•œ ì˜ˆì™¸ë¥¼ í¬í•¨í•œ ëª¨ë“  ì—ëŸ¬ë¥¼ ì²˜ë¦¬
            logger.error(f"Error getting related keywords for feed '{feed_id}': {e}", exc_info=True)
            return ErrorResponse(
                error=ErrorDetail(
                    code=ErrorCode.INTERNAL_ERROR,
                    message=Message.INTERNAL_ERROR
                )
            )